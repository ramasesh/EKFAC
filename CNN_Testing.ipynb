{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for testing EKFAC with Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing 1D Convolutional Neural Networks\n",
    "\n",
    "We start with a 1D CNN with a single layer.  In order to sensibly organize the derivatives with respect to the weights and biases, we try to get the input into a form such that we can write the output, $h$, as \n",
    "\\begin{equation}\n",
    "h_{iml} = \\sum_j A_{ijl}w_{mj}\n",
    "\\end{equation}\n",
    "Here, $w$ is the 'tensor' which represents the weights of the CNN.  The first index (denoted $m$ above) represents the the channel of the output layer, while the second index (denoted $j$ above) represents *both* the input channel and the spatial location (or offset).  So if we are used to thinking about the weight kernel as having three indices, one for output channel, one for input channel, and one for spatial location (or offset), the two-index $w_{mj}$ can be viewed as flattening the input channel + spatial location index into a single index. \n",
    "\n",
    "However, naturally the input is not in the correct form, but is actually organized by (batch_size, input_channel, input_spatial_location).  In order to convert this to the form above, that is, to the 'tensor' $A$, we need to organize it such that every element of the input which is 'seen' by a unique element (input channel x input spatial location) \n",
    "\n",
    "How do we do that?  \n",
    "\n",
    "First, we have to look at the shape of the weights for a convolutional layer.  The weight needs to have an index for the output channel, the input channel, and the spatial location (offset).  In PyTorch, the weight tensor is organized in that order: (output_channel x input_channel x spatial_location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_chans = 3\n",
    "n_output_chans = 2\n",
    "kernel_size = 4\n",
    "padding = 0\n",
    "stride = 1\n",
    "\n",
    "test_conv1d_module = torch.nn.Conv1d(in_channels=n_input_chans,\n",
    "                                     out_channels=n_output_chans,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     padding=padding,\n",
    "                                     stride=stride,\n",
    "                                     bias=False)\n",
    "\n",
    "test_conv1d_weight = test_conv1d_module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv1d_weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can play a trick.  To reshape the input into the form described above, with the $A_{ijl}$, we need to gather all of the elements that a particular output spatial location will 'see'.  So what we can do is make a fake convolutional weight, which pretends that the output actually has ``kernel_size * input_channels`` channels, where each of these 'channels' is simply one of the elements that the weight tensor interacts with.  Then, for each output spatial location, we simply dot-product this with the weight (as in the equation above).  \n",
    "\n",
    "**TODO Put Picture Here**\n",
    "\n",
    "So, we need to make a fake filter.  This is what the function below does.  Here's an example of what the fake-filter looks like if we have 3 input channels, a kernel size of 4:\n",
    "\n",
    "\n",
    "[[1,0,0,0],\n",
    " [0,0,0,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,1,0,0],\n",
    " [0,0,0,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,1,0],\n",
    " [0,0,0,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,0,1],\n",
    " [0,0,0,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [1,0,0,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,1,0,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,0,1,0],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,0,0,1],\n",
    " [0,0,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,0,0,0],\n",
    " [1,0,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,0,0,0],\n",
    " [0,1,0,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,0,0,0],\n",
    " [0,0,1,0]],\n",
    " \n",
    "[[0,0,0,0],\n",
    " [0,0,0,0],\n",
    " [0,0,0,1]],\n",
    "\n",
    "\n",
    "Viewing the filter in this way suggests an easy way to generate the filter.  Make the identity matrix of shape (kernel_size * input_channels, kernel_size * input_channels), and then reshape it to be (kernel_size * input_channels, input_channels, kernel_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gathering_filter(mod):\n",
    "    \"\"\"Convolution filter that extracts input patches.\"\"\"\n",
    "    dimension = len(mod.kernel_size)\n",
    "    if dimension == 1:\n",
    "        kernel_size, = mod.kernel_size\n",
    "\n",
    "        g_filter = torch.eye(kernel_size*mod.in_channels, \n",
    "                             dtype=mod.weight.dtype,\n",
    "                             device=mod.weight.device)\n",
    "        \n",
    "        return g_filter.view(kernel_size*mod.in_channels, \n",
    "                             mod.in_channels, \n",
    "                             kernel_size)     \n",
    "        \n",
    "    elif dimension == 2:\n",
    "        kernel_width, kernel_height = mod.kernel_size\n",
    "        \n",
    "        g_filter = torch.eye(kernel_width*kernel_height*mod.in_channels, \n",
    "                             dtype=mod.weight.dtype,\n",
    "                             device=mod.weight.device)\n",
    "        \n",
    "        return g_filter.view(kernel_width*kernel_height*mod.in_channels, \n",
    "                             mod.in_channels, \n",
    "                             kernel_width, \n",
    "                             kernel_height)\n",
    "        \n",
    "    elif dimension == 3:\n",
    "        kernel_width, kernel_height, kernel_depth = mod.kernel_size\n",
    "        \n",
    "        g_filter = torch.eye(kernel_width*kernel_height*kernel_depth*mod.in_channels, \n",
    "                             dtype=mod.weight.dtype,\n",
    "                             device=mod.weight.device)\n",
    "        \n",
    "        return g_filter.view(kernel_width*kernel_height*kernel_depth*mod.in_channels, \n",
    "                             mod.in_channels, \n",
    "                             kernel_width, \n",
    "                             kernel_height,\n",
    "                             kernel_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the gathering filters for convolutional 1D, 2D, and 3D network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of test: 2.384185791015625e-06\n",
      "Result of test: 1.430511474609375e-06\n",
      "Result of test: 9.5367431640625e-07\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 1.430511474609375e-06\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 1.3113021850585938e-06\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 1.430511474609375e-06\n",
      "Result of test: 1.430511474609375e-06\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 1.7881393432617188e-06\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 5.364418029785156e-07\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 1.0728836059570312e-06\n",
      "Result of test: 1.1920928955078125e-06\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 1.1920928955078125e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 8.344650268554688e-07\n",
      "Result of test: 1.7881393432617188e-06\n",
      "Result of test: 2.5331974029541016e-07\n",
      "Result of test: 7.748603820800781e-07\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 3.0994415283203125e-06\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 1.0728836059570312e-06\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 2.1457672119140625e-06\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 0.0\n",
      "Result of test: 2.086162567138672e-07\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 1.1920928955078125e-07\n",
      "Result of test: 0.0\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 8.344650268554688e-07\n",
      "Result of test: 1.1920928955078125e-07\n",
      "Result of test: 1.7881393432617188e-07\n",
      "Result of test: 5.960464477539062e-07\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 6.556510925292969e-07\n",
      "Result of test: 1.9073486328125e-06\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 7.450580596923828e-07\n",
      "Result of test: 9.5367431640625e-07\n",
      "Result of test: 1.5497207641601562e-06\n",
      "Result of test: 1.1920928955078125e-06\n",
      "Result of test: 2.0265579223632812e-06\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 1.0728836059570312e-06\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 0.0\n",
      "Result of test: 9.238719940185547e-07\n",
      "Result of test: 1.6689300537109375e-06\n",
      "Result of test: 1.5497207641601562e-06\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 6.556510925292969e-07\n",
      "Result of test: 1.2218952178955078e-06\n",
      "Result of test: 6.556510925292969e-07\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 5.364418029785156e-07\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 0.0\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 6.109476089477539e-07\n",
      "Result of test: 2.384185791015625e-07\n",
      "Result of test: 1.1920928955078125e-07\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 2.2649765014648438e-06\n",
      "Result of test: 3.5762786865234375e-07\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 4.76837158203125e-07\n",
      "Result of test: 0.0\n",
      "Result of test: 1.430511474609375e-06\n",
      "Result of test: 1.0728836059570312e-06\n",
      "Result of test: 1.1920928955078125e-07\n",
      "Result of test: 1.9371509552001953e-07\n",
      "Result of test: 7.152557373046875e-07\n",
      "Result of test: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "network_dim = 1\n",
    "\n",
    "num_tests = 100\n",
    "\n",
    "for _ in range(num_tests):\n",
    "    try:\n",
    "        Nbatch, input_dim, input_channels, output_channels = np.random.randint(1, 50, size=4)\n",
    "\n",
    "        kernel_size = np.random.randint(1, input_dim)\n",
    "\n",
    "        padding = np.random.randint(0, 5)\n",
    "        \n",
    "        stride = np.random.randint(1, input_dim)\n",
    "\n",
    "        \n",
    "        conv1d_model = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=input_channels,\n",
    "                            out_channels=output_channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            padding=padding,\n",
    "                            stride=stride,\n",
    "                            bias=False\n",
    "                            ))\n",
    "\n",
    "        input_1d = torch.randn(Nbatch, input_channels, input_dim)\n",
    "        weight_1d = list(conv1d_model.parameters())[0]\n",
    "\n",
    "        output_1d = conv1d_model(input_1d)\n",
    "        module = list(conv1d_model.modules())[1]\n",
    "        filter_1d = get_gathering_filter(module)\n",
    "\n",
    "        A = F.conv1d(input_1d, \n",
    "                     weight=filter_1d,\n",
    "                     padding=padding,\n",
    "                     stride=stride)\n",
    "\n",
    "        out_test = torch.einsum('ijl,mj->iml', A, weight_1d.view(output_channels, kernel_size * input_channels))\n",
    "\n",
    "        diff = out_test - output_1d\n",
    "        print('Result of test: {}'.format(torch.max(torch.abs(diff))))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 47, 23, 15])\n",
      "Test with following parameters\n",
      "Input X dimension: 29\n",
      "Input Y dimension: 26\n",
      "Batch size: 29\n",
      "Input channels: 47\n",
      "Output channels: 41\n",
      "Kernel X size: 23\n",
      "Kernel Y size: 15\n",
      "Padding: 2\n",
      "Stride: (25, 5)\n",
      "A size: torch.Size([29, 16215, 1, 4])\n",
      "Result of test: 1.8362115621566772\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    Nbatch = np.random.randint(1,50)\n",
    "    input_dimX = np.random.randint(1,50)\n",
    "    input_dimY = np.random.randint(1,50)\n",
    "    input_channels = np.random.randint(1,50)\n",
    "    output_channels = np.random.randint(1,50)\n",
    "\n",
    "    kernel_X = np.random.randint(1, input_dimX)\n",
    "    kernel_Y = np.random.randint(1, input_dimY)\n",
    "    kernel_size = (kernel_X, kernel_Y)\n",
    "\n",
    "    padding = np.random.randint(0, 5)\n",
    "    stride = (np.random.randint(1, input_dimX), np.random.randint(1, input_dimY))\n",
    "\n",
    "    conv2d_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=input_channels,\n",
    "                        out_channels=output_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        stride=stride,\n",
    "                        bias=False\n",
    "                        ))\n",
    "\n",
    "    input_2d = torch.randn(Nbatch, input_channels, input_dimX, input_dimY)\n",
    "    weight_2d = list(conv2d_model.parameters())[0]\n",
    "    print(weight_2d.size())\n",
    "\n",
    "    print('Test with following parameters')\n",
    "    print('Input X dimension: {}'.format(input_dimX))\n",
    "    print('Input Y dimension: {}'.format(input_dimY))\n",
    "    print('Batch size: {}'.format(Nbatch))\n",
    "    print('Input channels: {}'.format(input_channels))\n",
    "    print('Output channels: {}'.format(output_channels))\n",
    "    print('Kernel X size: {}'.format(kernel_X))\n",
    "    print('Kernel Y size: {}'.format(kernel_Y))\n",
    "    print('Padding: {}'.format(padding))\n",
    "    print('Stride: {}'.format(stride))\n",
    "\n",
    "    output_2d = conv2d_model(input_2d)\n",
    "    module = list(conv2d_model.modules())[1]\n",
    "    filter_2d = get_gathering_filter(module)\n",
    "\n",
    "    A = F.conv2d(input_2d, \n",
    "                 weight=filter_2d,\n",
    "                 padding=padding,\n",
    "                 stride=stride,\n",
    "                 groups = input_channels)\n",
    "    \n",
    "    print('A size: {}'.format(A.size()))\n",
    "    #         print('Weight size: {}'.format(weight_2d.size()))\n",
    "    #         print('Output size: {}'.format(output_2d.size()))\n",
    "\n",
    "    out_test = torch.einsum('ijkl,mj->imkl', A, weight_2d.view(output_channels, -1))\n",
    "\n",
    "    diff = out_test - output_2d\n",
    "    print('Result of test: {}'.format(torch.max(torch.abs(diff))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 45, 16, 43, 1])\n",
      "Test with following parameters\n",
      "Input X dimension: 18\n",
      "Input Y dimension: 46\n",
      "Input Z dimension: 2\n",
      "Batch size: 17\n",
      "Input channels: 45\n",
      "Output channels: 17\n",
      "Kernel X size: 16\n",
      "Kernel Y size: 43\n",
      "Kernel Z size: 1\n",
      "Padding: 2\n",
      "Stride: (15, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    Nbatch = np.random.randint(1,50)\n",
    "    input_dimX = np.random.randint(1,50)\n",
    "    input_dimY = np.random.randint(1,50)\n",
    "    input_dimZ = np.random.randint(1,50)\n",
    "    input_channels = np.random.randint(1,50)\n",
    "    output_channels = np.random.randint(1,50)\n",
    "\n",
    "    kernel_X = np.random.randint(1, input_dimX)\n",
    "    kernel_Y = np.random.randint(1, input_dimY)\n",
    "    kernel_Z = np.random.randint(1, input_dimZ)\n",
    "    kernel_size = (kernel_X, kernel_Y, kernel_Z)\n",
    "\n",
    "    padding = np.random.randint(0, 5)\n",
    "    stride = (np.random.randint(1, input_dimX), \n",
    "              np.random.randint(1, input_dimY),\n",
    "              np.random.randint(1, input_dimZ),\n",
    "             )\n",
    "\n",
    "    conv3d_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv3d(in_channels=input_channels,\n",
    "                        out_channels=output_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                        stride=stride,\n",
    "                        bias=False\n",
    "                        ))\n",
    "\n",
    "    input_3d = torch.randn(Nbatch, input_channels, input_dimX, input_dimY, input_dimZ)\n",
    "    weight_3d = list(conv3d_model.parameters())[0]\n",
    "    print(weight_3d.size())\n",
    "\n",
    "    print('Test with following parameters')\n",
    "    print('Input X dimension: {}'.format(input_dimX))\n",
    "    print('Input Y dimension: {}'.format(input_dimY))\n",
    "    print('Input Z dimension: {}'.format(input_dimZ))\n",
    "    print('Batch size: {}'.format(Nbatch))\n",
    "    print('Input channels: {}'.format(input_channels))\n",
    "    print('Output channels: {}'.format(output_channels))\n",
    "    print('Kernel X size: {}'.format(kernel_X))\n",
    "    print('Kernel Y size: {}'.format(kernel_Y))\n",
    "    print('Kernel Z size: {}'.format(kernel_Z))\n",
    "    print('Padding: {}'.format(padding))\n",
    "    print('Stride: {}'.format(stride))\n",
    "\n",
    "    output_3d = conv3d_model(input_3d)\n",
    "    module = list(conv3d_model.modules())[1]\n",
    "    filter_3d = get_gathering_filter(module)\n",
    "\n",
    "    A = F.conv3d(input_3d, \n",
    "                 weight=filter_3d,\n",
    "                 padding=padding,\n",
    "                 stride=stride)\n",
    "\n",
    "    print('A size: {}'.format(A.size()))\n",
    "    #         print('Weight size: {}'.format(weight_2d.size()))\n",
    "    #         print('Output size: {}'.format(output_2d.size()))\n",
    "\n",
    "    out_test = torch.einsum('ijklo,mj->imklo', A, weight_3d.view(output_channels, -1))\n",
    "\n",
    "    diff = out_test - output_3d\n",
    "    print('Result of test: {}'.format(torch.max(torch.abs(diff))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed attempt at making a sparse weight in the CNN \n",
    "Since the fake weight is so sparse, I thought it would be nice to make it a sparse Tensor in pyTorch, but somehow the sparse tensor is not working.  I guess pyTorch isn't used to using sparse tensors for CNNs, but in the future, maybe this is somethign good to do.  Here's the code:\n",
    "\n",
    "    def get_gathering_filter(mod):\n",
    "\n",
    "        dimension = len(mod.kernel_size)\n",
    "        if dimension == 1:        \n",
    "            kernel_size, = mod.kernel_size\n",
    "\n",
    "            indices_1 = np.arange(kernel_size*mod.in_channels)\n",
    "            indices_2 = indices_1 // kernel_size\n",
    "            indices_3 = indices_1 % kernel_size\n",
    "\n",
    "            nonzero_indices = torch.LongTensor(np.array([indices_1,\n",
    "                                        indices_2,\n",
    "                                        indices_3]))\n",
    "\n",
    "            values = torch.FloatTensor([1]*kernel_size*mod.in_channels)\n",
    "            filter_size = torch.Size([kernel_size*mod.in_channels, mod.in_channels, kernel_size])\n",
    "\n",
    "            return torch.sparse.FloatTensor(nonzero_indices, values, filter_size)\n",
    "   \n",
    "This has been verified to generate the right filter (by casting it to a dense tensor), so as soon as I figure out how to put a sparse weight in PyTorch, this should be good to go."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
